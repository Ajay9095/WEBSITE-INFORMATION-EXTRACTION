{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30050051",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 mysql-connector-python selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeddfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3244d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "import mysql.connector\n",
    "import re\n",
    "import langcodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329453d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#____________________________________________________________________________________________________________________________\n",
    "#\n",
    "#                                               EXTRACTING WEBSITE INFO\n",
    "#____________________________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_______________________________________________Initializing web driver______________________________________________________\n",
    "\n",
    "def extract_website_info(url):\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')                 \n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "                         AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    " \n",
    "                                                     # Prepend 'https://' if not present in the URL\n",
    "    if not url.startswith('http://') and not url.startswith('https://'):\n",
    "        url = f'https://{url}'\n",
    "        \n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading URL {url}: {e}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "#________________________________________Extract Meta Title and Description__________________________________________________\n",
    "\n",
    "    meta_title = soup.find('title').text if soup.find('title') else None\n",
    "    meta_description = soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else None\n",
    "\n",
    "    \n",
    "#__________________________________________Extract Social Media Links________________________________________________________\n",
    "\n",
    "    social_media_links = []\n",
    "    index = 1\n",
    "    added_platforms = set()      # Set to track added platforms\n",
    "     \n",
    "    platforms = [\n",
    "        'facebook', 'twitter', 'instagram', 'linkedin', 'youtube', 'x.com',   \n",
    "        'whatsapp', 'reddit', 'play.google', 'telegram', 'tiktok', 'pinterest', 'github'\n",
    "    ]\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        link_url = link['href'].lower()\n",
    "        for platform in platforms:\n",
    "            if platform in link_url and platform not in added_platforms:\n",
    "                social_media_links.append(f\"{index}. {link['href']}\")\n",
    "                index += 1\n",
    "                added_platforms.add(platform)      # Add platform to set\n",
    "                \n",
    "    if not social_media_links:\n",
    "        social_media_links.append(\"Not Specified\")\n",
    "        \n",
    "\n",
    "#____________________________________________Extract Tech Stack____________________________________________________________\n",
    "\n",
    "    tech_stack = []\n",
    "    \n",
    "#------------------Detect MVC frameworks--------------------\n",
    "\n",
    "    mvc_frameworks = [\n",
    "        'angular', 'react', 'vue', 'ember', 'backbone', 'knockout', 'svelte', \n",
    "        'meteor', 'aurelia', 'marionette', 'mithril', 'riot', 'polymer', \n",
    "        'alpine', 'next', 'nuxt', 'blazor', 'asp.net'\n",
    "    ]\n",
    "    \n",
    "    detected_mvc_frameworks = []\n",
    "            \n",
    "    for framework in mvc_frameworks:\n",
    "        if soup.find_all('script', src=lambda src: src and framework in src.lower()):\n",
    "            detected_mvc_frameworks.append(framework.capitalize())\n",
    "            \n",
    "    if detected_mvc_frameworks:\n",
    "        tech_stack.append(f\"MVC Framework ({', '.join(detected_mvc_frameworks)})\")\n",
    "\n",
    "        \n",
    "#------------------Detect CMS platforms-----------------------\n",
    "\n",
    "    cms_keywords = {\n",
    "        'WordPress': ['wp-content', 'wp-include', 'wp-json'],\n",
    "        'Shopify': ['shopify', 'cdn.shopify'],\n",
    "        'Joomla': ['joomla', 'joomla!'],\n",
    "        'Magento': ['magento', 'mage'],\n",
    "        'Drupal': ['drupal', 'sites/default'],\n",
    "        'Wix': ['wix.com', 'wixsite'],\n",
    "        'Squarespace': ['squarespace', 'sqspcdn'],\n",
    "        'PrestaShop': ['prestashop', 'ps_version'],\n",
    "        'TYPO3': ['typo3', 't3lib'],\n",
    "        'Blogger': ['blogger', 'blogspot'],\n",
    "        'BigCommerce': ['bigcommerce', 'bc-sf-filter'],\n",
    "        'Ghost': ['ghost', 'gh-content'],\n",
    "        'HubSpot': ['hubspot', 'hs-scripts'],\n",
    "        'ExpressionEngine': ['exp:channel', 'expressionengine'],\n",
    "        'Craft CMS': ['craft', 'craftcms'],\n",
    "        'Sitecore': ['sitecore', 'sc_webedit'],\n",
    "        'Concrete5': ['concrete5', 'ccm_app']\n",
    "    }\n",
    "    \n",
    "    detected_cms = []\n",
    "    for cms, keywords in cms_keywords.items():\n",
    "        if any(keyword in str(soup).lower() for keyword in keywords):\n",
    "            detected_cms.append(cms)\n",
    "            \n",
    "    if detected_cms:\n",
    "        tech_stack.append(f'CMS ({\", \".join(detected_cms)})')\n",
    "        \n",
    "        \n",
    "#----- --------Detect JavaScript libraries and frameworks--------------------   \n",
    "\n",
    "    js_libraries = [\n",
    "        'jquery', 'bootstrap', 'd3', 'underscore', 'lodash', 'moment', \n",
    "        'chart.js','three.js', 'anime.js', 'axios', 'rxjs', 'handlebars', \n",
    "        'mustache', 'p5.js', 'paper.js', 'raphael', 'pixi.js', 'gsap', \n",
    "        'velocity.js','dojo', 'ext.js', 'alpine'\n",
    "    ]\n",
    "    \n",
    "    detected_js_libraries = []\n",
    "    \n",
    "    for lib in js_libraries:\n",
    "        if soup.find_all('script', src=lambda src: src and lib in src.lower()):\n",
    "            detected_js_libraries.append(lib.capitalize())\n",
    "            \n",
    "    if detected_js_libraries:\n",
    "        tech_stack.append(f\"JavaScript Library ({', '.join(detected_js_libraries)})\")\n",
    "        \n",
    "        \n",
    "#---------------------------Detect CSS frameworks------------------------------\n",
    "\n",
    "    css_frameworks = [\n",
    "        'bootstrap', 'foundation', 'bulma', 'tailwind', 'materialize', 'semantic ui', \n",
    "        'pure', 'milligram', 'uikit', 'spectre.css', 'skeleton', 'basscss', 'siimple', \n",
    "        'susy', 'water.css', 'tachyons', 'primer', 'chota', 'min.css', 'blaze', 'wing', \n",
    "        'rscss', 'yaml', 'ink', 'cutestrap', 'baseline', 'metroui', 'topcoat', 'solarcss'\n",
    "    ]\n",
    "    \n",
    "    detected_css_frameworks = []\n",
    "    \n",
    "    for framework in css_frameworks:\n",
    "        if soup.find_all('link', rel='stylesheet', href=lambda href: href and framework in href.lower()):\n",
    "            detected_css_frameworks.append(framework.capitalize())\n",
    "            \n",
    "    if detected_css_frameworks:\n",
    "        tech_stack.append(f\"CSS Framework ({', '.join(detected_css_frameworks)})\")\n",
    "        \n",
    "        \n",
    "#---------------------Detect Backend frameworks and languages----------------------\n",
    "\n",
    "    backend_tech = [\n",
    "        'django', 'flask', 'express', 'laravel', 'spring', 'ruby on rails', 'node.js', \n",
    "        'php', 'java', 'python', 'ruby', 'asp.net', 'dotnet', 'go', 'scala', 'elixir', \n",
    "        'perl', 'rust', 'haskell', 'swift', 'typescript', 'clojure', 'r', 'kotlin'\n",
    "    ]\n",
    "    \n",
    "    detected_backend_tech = []\n",
    "    \n",
    "    for tech in backend_tech:\n",
    "        if soup.find_all('script', src=lambda src: src and tech in src.lower()):\n",
    "            detected_backend_tech.append(tech.capitalize())\n",
    "    \n",
    "    if detected_backend_tech:\n",
    "        tech_stack.append(f\"Backend Technology ({', '.join(detected_backend_tech)})\")\n",
    "        \n",
    "\n",
    "#__________________________________________Extract Payment Gateways_________________________________________________________                 \n",
    "            \n",
    "    payment_gateways = []\n",
    "\n",
    "    page_text = soup.get_text()                                                          # Extracting all text from the page\n",
    "\n",
    "    \n",
    "    payment_patterns = [\n",
    "        r'\\bpaypal\\b', r'\\bstripe\\b', r'\\brazorpay\\b',r'\\bsquare\\b',r'\\bauthorize.net\\b',\n",
    "        r'\\bworldpay\\b',r'\\b2checkout\\b',r'\\bbraintree\\b',r'\\badyen\\b',r'\\bvenmo\\b',                # List of common paymentS\n",
    "        r'\\bapple pay\\b',r'\\bgoogle pay\\b',r'\\bpayoneer\\b',r'\\bskrill\\b',r'\\bwechat pay\\b',\n",
    "        r'\\balipay\\b',r'\\bpaytm\\b',r'\\bzelle\\b',r'\\bklarna\\b',r'\\bpaytm\\b',r'\\btransferwise\\b',\n",
    "        r'\\bneteller\\b', r'\\bsquare cash\\b'\n",
    "    ]\n",
    "\n",
    "    detected_gateways = set()\n",
    "\n",
    "    for pattern in payment_patterns:\n",
    "        if re.search(pattern, page_text, re.IGNORECASE):                                                # Search for payment \n",
    "            detected_gateways.add(re.search(pattern, page_text, re.IGNORECASE).group(0).capitalize())\n",
    "\n",
    "    payment_gateways = list(detected_gateways) if detected_gateways else ['Not specified']\n",
    "    \n",
    "\n",
    "#___________________________________________Extract Website Language________________________________________________________\n",
    "\n",
    "    lang_code = soup.find('html')['lang'] if soup.find('html') and 'lang' in soup.find('html').attrs else None\n",
    "    website_language = langcodes.get(lang_code).language_name() if lang_code else None\n",
    "    \n",
    "\n",
    "#_______________________________________Website Category Based on Content___________________________________________________\n",
    "\n",
    "    categories = {\n",
    "        'E-commerce': ['shop', 'cart', 'checkout', 'product', 'sale', 'buy', 'ecommerce', 'online store', 'shopping', 'retail'],\n",
    "        'Blog': ['blog', 'post', 'article', 'comment', 'author', 'subscribe'],\n",
    "        'News': ['news', 'breaking', 'headline', 'report', 'journal', 'media'],\n",
    "        'Education': ['course', 'lesson', 'education', 'school', 'university', 'college', 'study', 'lecture'],\n",
    "        'Technology': ['tech', 'software', 'gadget', 'programming', 'developer', 'IT'],\n",
    "        'Health': ['health', 'medicine', 'doctor', 'hospital', 'clinic', 'fitness', 'wellness'],\n",
    "        'Finance': ['finance', 'bank', 'investment', 'loan', 'insurance', 'account', 'money', 'stock'],\n",
    "        'Travel': ['travel', 'trip', 'tour', 'destination', 'flight', 'hotel', 'vacation'],\n",
    "        'Food': ['food', 'recipe', 'restaurant', 'cooking', 'cuisine', 'dish'],\n",
    "        'Real Estate': ['real estate', 'property', 'home', 'apartment', 'house', 'land', 'broker'],\n",
    "        'Entertainment': ['entertainment', 'movie', 'film', 'music', 'game', 'concert', 'celebrity'],\n",
    "        'Sports': ['sports', 'fitness', 'exercise', 'athlete', 'tournament', 'championship'],\n",
    "        'Art': ['art', 'painting', 'sculpture', 'gallery', 'artist', 'creative', 'design'],\n",
    "        'Government': ['government', 'official', 'public', 'policy', 'law', 'regulation'],\n",
    "        'Automotive': ['automotive', 'car', 'vehicle', 'auto', 'truck', 'motor', 'parts'],\n",
    "        'Charity': ['charity', 'nonprofit', 'donate', 'volunteer', 'foundation', 'cause'],\n",
    "        'Music': ['music', 'band', 'concert', 'album', 'song', 'musician'],\n",
    "        'Photography': ['photography', 'photo', 'photographer', 'image', 'picture', 'camera'],\n",
    "        'Fashion': ['fashion', 'style', 'clothing', 'apparel', 'trend', 'designer'],\n",
    "        'Gaming': ['gaming', 'video game', 'esports', 'gamer', 'console', 'streaming'],\n",
    "        'Law': ['law', 'legal', 'attorney', 'court', 'justice', 'lawyer'],\n",
    "        'Science': ['science', 'research', 'scientist', 'biology', 'physics', 'chemistry']\n",
    "    }\n",
    "\n",
    "    \n",
    "    text_content = ' '.join([tag.get_text().lower() for tag in soup.find_all(['title', 'meta', 'h1', 'h2', 'h3', 'p'])])\n",
    "\n",
    "    category_scores = {category: 0 for category in categories}           # Initializing a score dictionary\n",
    "\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:                                # Counting the occurrences of each keyword in the text content\n",
    "            category_scores[category] += text_content.count(keyword)\n",
    "\n",
    "    primary_category = max(category_scores, key=category_scores.get)    # Selecting the category with the highest score\n",
    "\n",
    "    \n",
    "    if category_scores[primary_category] == 0:\n",
    "        category = 'General'\n",
    "    else:\n",
    "        category = primary_category\n",
    "        \n",
    "#____________________________________________Returning All the Values_______________________________________________________   \n",
    "\n",
    "    return {\n",
    "        'url': url,\n",
    "        'social_media_links': '\\n\\n\\n'.join(social_media_links),\n",
    "        'tech_stack': ',\\n\\n'.join(tech_stack),\n",
    "        'meta_title': meta_title,\n",
    "        'meta_description': meta_description,\n",
    "        'payment_gateways': ','.join(payment_gateways),\n",
    "        'website_language': website_language,\n",
    "        'category': category\n",
    "    }\n",
    "\n",
    "#____________________________________Connecting And Saving to MySQl Database _______________________________________________\n",
    "\n",
    "def save_to_database(data):\n",
    "    conn = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='@jaykumar_A04',   \n",
    "        database='web_scraping'\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO website_info (url, social_media_links, tech_stack, meta_title, meta_description, payment_gateways, website_language, category)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (data['url'], data['social_media_links'], data['tech_stack'], data['meta_title'], data['meta_description'], data['payment_gateways'], data['website_language'], data['category']))\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "#__________________________________________________Main Function_______________________________________________________________\n",
    "\n",
    "def main():\n",
    "    urls = ['www.udacity.com']        # Paste your urls\n",
    "    for url in urls:\n",
    "        try:\n",
    "            data = extract_website_info(url)\n",
    "            if data:\n",
    "                save_to_database(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "#____________________________________________________________________________________________________________________________\n",
    "#\n",
    "#                                                    THE END\n",
    "#____________________________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6845d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d7dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "www.coursera.org\n",
    "www.khanacademy.org\n",
    "www.codecademy.com\n",
    "ocw.mit.edu\n",
    "www.duolingo.com\n",
    "ed.ted.com\n",
    "www.skillshare.com\n",
    "www.linkedin.com/learning\n",
    "finance.yahoo.com\n",
    "www.investopedia.com\n",
    "www.bloomberg.com\n",
    "www.cnbc.com\n",
    "www.fool.com\n",
    "www.marketwatch.com\n",
    "www.seekingalpha.com\n",
    "www.morningstar.com\n",
    "www.nerdwallet.com\n",
    "www.ft.com\n",
    "www.espn.com\n",
    "www.cbssports.com\n",
    "bleacherreport.com\n",
    "www.si.com\n",
    "www.foxsports.com\n",
    "www.nbcsports.com\n",
    "sports.yahoo.com\n",
    "www.nfl.com\n",
    "www.nba.com\n",
    "www.mlb.com\n",
    "www.cnn.com\n",
    "www.bbc.com/news\n",
    "www.reuters.com\n",
    "www.nytimes.com\n",
    "www.theguardian.com\n",
    "www.washingtonpost.com\n",
    "www.aljazeera.com\n",
    "www.npr.org\n",
    "www.apnews.com\n",
    "www.usatoday.com\n",
    "techcrunch.com\n",
    "www.theverge.com\n",
    "www.wired.com\n",
    "www.cnet.com\n",
    "mashable.com\n",
    "gizmodo.com\n",
    "www.engadget.com\n",
    "arstechnica.com\n",
    "slashdot.org\n",
    "www.tomshardware.com\n",
    "www.webmd.com\n",
    "www.mayoclinic.org\n",
    "www.cdc.gov\n",
    "www.healthline.com\n",
    "www.nih.gov\n",
    "www.medicalnewstoday.com\n",
    "www.everydayhealth.com\n",
    "www.nhs.uk\n",
    "www.health.com\n",
    "www.who.int\n",
    "www.spotify.com\n",
    "www.apple.com/music\n",
    "www.soundcloud.com\n",
    "www.pandora.com\n",
    "www.youtube.com/music\n",
    "www.deezer.com\n",
    "www.tidal.com\n",
    "www.bandcamp.com\n",
    "www.mixcloud.com\n",
    "www.billboard.com/music\n",
    "www.autotrader.com\n",
    "www.cars.com\n",
    "www.edmunds.com\n",
    "www.kbb.com\n",
    "www.caranddriver.com\n",
    "www.motortrend.com\n",
    "www.autoblog.com\n",
    "www.thedrive.com\n",
    "www.autocar.co.uk\n",
    "www.hemmings.com\n",
    "www.allrecipes.com\n",
    "www.foodnetwork.com\n",
    "www.epicurious.com\n",
    "www.bonappetit.com\n",
    "www.seriouseats.com\n",
    "www.yummly.com\n",
    "www.thekitchn.com\n",
    "www.tasteofhome.com\n",
    "www.delish.com\n",
    "www.eater.com\n",
    "www.vogue.com\n",
    "www.elle.com\n",
    "www.harpersbazaar.com\n",
    "www.thecut.com\n",
    "www.gq.com\n",
    "www.glamour.com\n",
    "www.refinery29.com\n",
    "www.instyle.com\n",
    "www.cosmopolitan.com\n",
    "www.marieclaire.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2829a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
